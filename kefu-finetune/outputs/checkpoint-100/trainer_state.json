{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.013373900832525328,
  "eval_steps": 500,
  "global_step": 100,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00013373900832525326,
      "grad_norm": 9.48095703125,
      "learning_rate": 0.0,
      "loss": 5.9882,
      "step": 1
    },
    {
      "epoch": 0.0002674780166505065,
      "grad_norm": 7.395937442779541,
      "learning_rate": 4e-05,
      "loss": 5.9761,
      "step": 2
    },
    {
      "epoch": 0.0004012170249757598,
      "grad_norm": 8.62553596496582,
      "learning_rate": 8e-05,
      "loss": 5.817,
      "step": 3
    },
    {
      "epoch": 0.000534956033301013,
      "grad_norm": 6.919680118560791,
      "learning_rate": 0.00012,
      "loss": 5.1312,
      "step": 4
    },
    {
      "epoch": 0.0006686950416262663,
      "grad_norm": 4.771094799041748,
      "learning_rate": 0.00016,
      "loss": 4.3483,
      "step": 5
    },
    {
      "epoch": 0.0008024340499515196,
      "grad_norm": 2.1990256309509277,
      "learning_rate": 0.0002,
      "loss": 3.4834,
      "step": 6
    },
    {
      "epoch": 0.0009361730582767728,
      "grad_norm": 2.0380921363830566,
      "learning_rate": 0.00019789473684210526,
      "loss": 3.6612,
      "step": 7
    },
    {
      "epoch": 0.001069912066602026,
      "grad_norm": 1.6952707767486572,
      "learning_rate": 0.00019578947368421054,
      "loss": 3.2447,
      "step": 8
    },
    {
      "epoch": 0.0012036510749272795,
      "grad_norm": 1.4505749940872192,
      "learning_rate": 0.0001936842105263158,
      "loss": 3.1398,
      "step": 9
    },
    {
      "epoch": 0.0013373900832525326,
      "grad_norm": 1.3861089944839478,
      "learning_rate": 0.00019157894736842104,
      "loss": 3.1863,
      "step": 10
    },
    {
      "epoch": 0.001471129091577786,
      "grad_norm": 1.289580225944519,
      "learning_rate": 0.00018947368421052632,
      "loss": 3.2369,
      "step": 11
    },
    {
      "epoch": 0.0016048680999030391,
      "grad_norm": 1.1206341981887817,
      "learning_rate": 0.0001873684210526316,
      "loss": 3.0325,
      "step": 12
    },
    {
      "epoch": 0.0017386071082282925,
      "grad_norm": 0.9981635212898254,
      "learning_rate": 0.00018526315789473685,
      "loss": 3.0056,
      "step": 13
    },
    {
      "epoch": 0.0018723461165535457,
      "grad_norm": 1.0740450620651245,
      "learning_rate": 0.0001831578947368421,
      "loss": 2.806,
      "step": 14
    },
    {
      "epoch": 0.002006085124878799,
      "grad_norm": 1.0716392993927002,
      "learning_rate": 0.00018105263157894739,
      "loss": 2.6505,
      "step": 15
    },
    {
      "epoch": 0.002139824133204052,
      "grad_norm": 0.9850137829780579,
      "learning_rate": 0.00017894736842105264,
      "loss": 2.748,
      "step": 16
    },
    {
      "epoch": 0.0022735631415293053,
      "grad_norm": 1.078697681427002,
      "learning_rate": 0.0001768421052631579,
      "loss": 2.9607,
      "step": 17
    },
    {
      "epoch": 0.002407302149854559,
      "grad_norm": 0.867089569568634,
      "learning_rate": 0.00017473684210526317,
      "loss": 2.2841,
      "step": 18
    },
    {
      "epoch": 0.002541041158179812,
      "grad_norm": 1.0531272888183594,
      "learning_rate": 0.00017263157894736842,
      "loss": 2.7396,
      "step": 19
    },
    {
      "epoch": 0.0026747801665050652,
      "grad_norm": 0.9562149047851562,
      "learning_rate": 0.0001705263157894737,
      "loss": 2.6675,
      "step": 20
    },
    {
      "epoch": 0.002808519174830319,
      "grad_norm": 0.832094669342041,
      "learning_rate": 0.00016842105263157895,
      "loss": 2.497,
      "step": 21
    },
    {
      "epoch": 0.002942258183155572,
      "grad_norm": 0.9016155004501343,
      "learning_rate": 0.00016631578947368423,
      "loss": 2.4039,
      "step": 22
    },
    {
      "epoch": 0.003075997191480825,
      "grad_norm": 0.9031435251235962,
      "learning_rate": 0.00016421052631578948,
      "loss": 2.5241,
      "step": 23
    },
    {
      "epoch": 0.0032097361998060783,
      "grad_norm": 1.0795705318450928,
      "learning_rate": 0.00016210526315789473,
      "loss": 2.6184,
      "step": 24
    },
    {
      "epoch": 0.003343475208131332,
      "grad_norm": 0.8435359001159668,
      "learning_rate": 0.00016,
      "loss": 2.3412,
      "step": 25
    },
    {
      "epoch": 0.003477214216456585,
      "grad_norm": 1.0009965896606445,
      "learning_rate": 0.00015789473684210527,
      "loss": 2.3694,
      "step": 26
    },
    {
      "epoch": 0.003610953224781838,
      "grad_norm": 0.8872036337852478,
      "learning_rate": 0.00015578947368421052,
      "loss": 2.2516,
      "step": 27
    },
    {
      "epoch": 0.0037446922331070913,
      "grad_norm": 1.0844444036483765,
      "learning_rate": 0.0001536842105263158,
      "loss": 2.4095,
      "step": 28
    },
    {
      "epoch": 0.003878431241432345,
      "grad_norm": 1.2137197256088257,
      "learning_rate": 0.00015157894736842108,
      "loss": 2.5426,
      "step": 29
    },
    {
      "epoch": 0.004012170249757598,
      "grad_norm": 0.8557159304618835,
      "learning_rate": 0.00014947368421052633,
      "loss": 2.431,
      "step": 30
    },
    {
      "epoch": 0.004145909258082852,
      "grad_norm": 1.0433440208435059,
      "learning_rate": 0.00014736842105263158,
      "loss": 2.1248,
      "step": 31
    },
    {
      "epoch": 0.004279648266408104,
      "grad_norm": 1.0436798334121704,
      "learning_rate": 0.00014526315789473686,
      "loss": 2.443,
      "step": 32
    },
    {
      "epoch": 0.004413387274733358,
      "grad_norm": 1.000441312789917,
      "learning_rate": 0.0001431578947368421,
      "loss": 2.283,
      "step": 33
    },
    {
      "epoch": 0.004547126283058611,
      "grad_norm": 1.1605725288391113,
      "learning_rate": 0.00014105263157894736,
      "loss": 2.1042,
      "step": 34
    },
    {
      "epoch": 0.004680865291383864,
      "grad_norm": 1.3040509223937988,
      "learning_rate": 0.00013894736842105264,
      "loss": 2.3285,
      "step": 35
    },
    {
      "epoch": 0.004814604299709118,
      "grad_norm": 1.0376605987548828,
      "learning_rate": 0.0001368421052631579,
      "loss": 2.0653,
      "step": 36
    },
    {
      "epoch": 0.004948343308034371,
      "grad_norm": 1.1813809871673584,
      "learning_rate": 0.00013473684210526317,
      "loss": 2.0235,
      "step": 37
    },
    {
      "epoch": 0.005082082316359624,
      "grad_norm": 0.9163125157356262,
      "learning_rate": 0.00013263157894736842,
      "loss": 2.3229,
      "step": 38
    },
    {
      "epoch": 0.005215821324684878,
      "grad_norm": 1.0208460092544556,
      "learning_rate": 0.0001305263157894737,
      "loss": 2.2324,
      "step": 39
    },
    {
      "epoch": 0.0053495603330101305,
      "grad_norm": 1.03745436668396,
      "learning_rate": 0.00012842105263157895,
      "loss": 2.1488,
      "step": 40
    },
    {
      "epoch": 0.005483299341335384,
      "grad_norm": 1.0271953344345093,
      "learning_rate": 0.0001263157894736842,
      "loss": 2.089,
      "step": 41
    },
    {
      "epoch": 0.005617038349660638,
      "grad_norm": 0.9883462190628052,
      "learning_rate": 0.00012421052631578949,
      "loss": 2.2631,
      "step": 42
    },
    {
      "epoch": 0.00575077735798589,
      "grad_norm": 1.0464872121810913,
      "learning_rate": 0.00012210526315789474,
      "loss": 1.909,
      "step": 43
    },
    {
      "epoch": 0.005884516366311144,
      "grad_norm": 1.0623066425323486,
      "learning_rate": 0.00012,
      "loss": 2.2025,
      "step": 44
    },
    {
      "epoch": 0.006018255374636397,
      "grad_norm": 0.9307968616485596,
      "learning_rate": 0.00011789473684210525,
      "loss": 2.1585,
      "step": 45
    },
    {
      "epoch": 0.00615199438296165,
      "grad_norm": 1.0023690462112427,
      "learning_rate": 0.00011578947368421053,
      "loss": 1.9907,
      "step": 46
    },
    {
      "epoch": 0.006285733391286904,
      "grad_norm": 0.9358095526695251,
      "learning_rate": 0.0001136842105263158,
      "loss": 2.0433,
      "step": 47
    },
    {
      "epoch": 0.006419472399612157,
      "grad_norm": 0.9066495299339294,
      "learning_rate": 0.00011157894736842105,
      "loss": 1.8542,
      "step": 48
    },
    {
      "epoch": 0.00655321140793741,
      "grad_norm": 1.2684457302093506,
      "learning_rate": 0.00010947368421052633,
      "loss": 2.1795,
      "step": 49
    },
    {
      "epoch": 0.006686950416262664,
      "grad_norm": 1.222724437713623,
      "learning_rate": 0.00010736842105263158,
      "loss": 2.264,
      "step": 50
    },
    {
      "epoch": 0.0068206894245879165,
      "grad_norm": 0.9820836782455444,
      "learning_rate": 0.00010526315789473685,
      "loss": 2.0516,
      "step": 51
    },
    {
      "epoch": 0.00695442843291317,
      "grad_norm": 0.9892033338546753,
      "learning_rate": 0.00010315789473684211,
      "loss": 2.5183,
      "step": 52
    },
    {
      "epoch": 0.007088167441238423,
      "grad_norm": 0.8911715149879456,
      "learning_rate": 0.00010105263157894738,
      "loss": 1.981,
      "step": 53
    },
    {
      "epoch": 0.007221906449563676,
      "grad_norm": 1.2955937385559082,
      "learning_rate": 9.894736842105263e-05,
      "loss": 1.9305,
      "step": 54
    },
    {
      "epoch": 0.00735564545788893,
      "grad_norm": 1.0150350332260132,
      "learning_rate": 9.68421052631579e-05,
      "loss": 2.0747,
      "step": 55
    },
    {
      "epoch": 0.007489384466214183,
      "grad_norm": 0.9551563858985901,
      "learning_rate": 9.473684210526316e-05,
      "loss": 1.836,
      "step": 56
    },
    {
      "epoch": 0.007623123474539436,
      "grad_norm": 0.9767833352088928,
      "learning_rate": 9.263157894736843e-05,
      "loss": 2.0543,
      "step": 57
    },
    {
      "epoch": 0.00775686248286469,
      "grad_norm": 1.0607208013534546,
      "learning_rate": 9.052631578947369e-05,
      "loss": 1.967,
      "step": 58
    },
    {
      "epoch": 0.007890601491189943,
      "grad_norm": 0.9671193957328796,
      "learning_rate": 8.842105263157894e-05,
      "loss": 1.7353,
      "step": 59
    },
    {
      "epoch": 0.008024340499515196,
      "grad_norm": 1.0022255182266235,
      "learning_rate": 8.631578947368421e-05,
      "loss": 2.0164,
      "step": 60
    },
    {
      "epoch": 0.008158079507840449,
      "grad_norm": 1.274146556854248,
      "learning_rate": 8.421052631578948e-05,
      "loss": 1.8711,
      "step": 61
    },
    {
      "epoch": 0.008291818516165703,
      "grad_norm": 0.9455798864364624,
      "learning_rate": 8.210526315789474e-05,
      "loss": 1.7305,
      "step": 62
    },
    {
      "epoch": 0.008425557524490956,
      "grad_norm": 1.151148796081543,
      "learning_rate": 8e-05,
      "loss": 1.5655,
      "step": 63
    },
    {
      "epoch": 0.008559296532816209,
      "grad_norm": 1.1299461126327515,
      "learning_rate": 7.789473684210526e-05,
      "loss": 1.8603,
      "step": 64
    },
    {
      "epoch": 0.008693035541141463,
      "grad_norm": 1.0780693292617798,
      "learning_rate": 7.578947368421054e-05,
      "loss": 2.0948,
      "step": 65
    },
    {
      "epoch": 0.008826774549466716,
      "grad_norm": 1.1548868417739868,
      "learning_rate": 7.368421052631579e-05,
      "loss": 1.871,
      "step": 66
    },
    {
      "epoch": 0.008960513557791969,
      "grad_norm": 0.9224773049354553,
      "learning_rate": 7.157894736842105e-05,
      "loss": 1.8908,
      "step": 67
    },
    {
      "epoch": 0.009094252566117221,
      "grad_norm": 1.0834871530532837,
      "learning_rate": 6.947368421052632e-05,
      "loss": 2.1361,
      "step": 68
    },
    {
      "epoch": 0.009227991574442476,
      "grad_norm": 1.0182980298995972,
      "learning_rate": 6.736842105263159e-05,
      "loss": 1.9721,
      "step": 69
    },
    {
      "epoch": 0.009361730582767729,
      "grad_norm": 1.0407838821411133,
      "learning_rate": 6.526315789473685e-05,
      "loss": 1.7253,
      "step": 70
    },
    {
      "epoch": 0.009495469591092981,
      "grad_norm": 0.9961434006690979,
      "learning_rate": 6.31578947368421e-05,
      "loss": 1.9915,
      "step": 71
    },
    {
      "epoch": 0.009629208599418236,
      "grad_norm": 0.9746225476264954,
      "learning_rate": 6.105263157894737e-05,
      "loss": 2.0108,
      "step": 72
    },
    {
      "epoch": 0.009762947607743488,
      "grad_norm": 0.8111729621887207,
      "learning_rate": 5.894736842105263e-05,
      "loss": 1.5149,
      "step": 73
    },
    {
      "epoch": 0.009896686616068741,
      "grad_norm": 0.902398407459259,
      "learning_rate": 5.68421052631579e-05,
      "loss": 2.045,
      "step": 74
    },
    {
      "epoch": 0.010030425624393996,
      "grad_norm": 1.1224887371063232,
      "learning_rate": 5.4736842105263165e-05,
      "loss": 2.0211,
      "step": 75
    },
    {
      "epoch": 0.010164164632719248,
      "grad_norm": 1.0125305652618408,
      "learning_rate": 5.2631578947368424e-05,
      "loss": 1.6954,
      "step": 76
    },
    {
      "epoch": 0.010297903641044501,
      "grad_norm": 1.1617871522903442,
      "learning_rate": 5.052631578947369e-05,
      "loss": 1.9415,
      "step": 77
    },
    {
      "epoch": 0.010431642649369756,
      "grad_norm": 0.9481138586997986,
      "learning_rate": 4.842105263157895e-05,
      "loss": 1.7334,
      "step": 78
    },
    {
      "epoch": 0.010565381657695008,
      "grad_norm": 1.1176338195800781,
      "learning_rate": 4.6315789473684214e-05,
      "loss": 1.8269,
      "step": 79
    },
    {
      "epoch": 0.010699120666020261,
      "grad_norm": 1.063260793685913,
      "learning_rate": 4.421052631578947e-05,
      "loss": 1.9984,
      "step": 80
    },
    {
      "epoch": 0.010832859674345515,
      "grad_norm": 1.1164087057113647,
      "learning_rate": 4.210526315789474e-05,
      "loss": 1.9978,
      "step": 81
    },
    {
      "epoch": 0.010966598682670768,
      "grad_norm": 1.4433066844940186,
      "learning_rate": 4e-05,
      "loss": 1.5704,
      "step": 82
    },
    {
      "epoch": 0.011100337690996021,
      "grad_norm": 0.9471150636672974,
      "learning_rate": 3.789473684210527e-05,
      "loss": 1.8099,
      "step": 83
    },
    {
      "epoch": 0.011234076699321275,
      "grad_norm": 1.1712385416030884,
      "learning_rate": 3.578947368421053e-05,
      "loss": 1.8114,
      "step": 84
    },
    {
      "epoch": 0.011367815707646528,
      "grad_norm": 0.9529018402099609,
      "learning_rate": 3.368421052631579e-05,
      "loss": 1.8864,
      "step": 85
    },
    {
      "epoch": 0.01150155471597178,
      "grad_norm": 1.233909249305725,
      "learning_rate": 3.157894736842105e-05,
      "loss": 2.1531,
      "step": 86
    },
    {
      "epoch": 0.011635293724297033,
      "grad_norm": 1.0358158349990845,
      "learning_rate": 2.9473684210526314e-05,
      "loss": 2.0235,
      "step": 87
    },
    {
      "epoch": 0.011769032732622288,
      "grad_norm": 1.313896894454956,
      "learning_rate": 2.7368421052631583e-05,
      "loss": 1.5958,
      "step": 88
    },
    {
      "epoch": 0.01190277174094754,
      "grad_norm": 1.0787088871002197,
      "learning_rate": 2.5263157894736845e-05,
      "loss": 1.9403,
      "step": 89
    },
    {
      "epoch": 0.012036510749272793,
      "grad_norm": 1.4339745044708252,
      "learning_rate": 2.3157894736842107e-05,
      "loss": 1.6869,
      "step": 90
    },
    {
      "epoch": 0.012170249757598048,
      "grad_norm": 1.1636483669281006,
      "learning_rate": 2.105263157894737e-05,
      "loss": 1.8506,
      "step": 91
    },
    {
      "epoch": 0.0123039887659233,
      "grad_norm": 0.9245219826698303,
      "learning_rate": 1.8947368421052634e-05,
      "loss": 1.8866,
      "step": 92
    },
    {
      "epoch": 0.012437727774248553,
      "grad_norm": 1.1891719102859497,
      "learning_rate": 1.6842105263157896e-05,
      "loss": 1.7506,
      "step": 93
    },
    {
      "epoch": 0.012571466782573808,
      "grad_norm": 0.9172110557556152,
      "learning_rate": 1.4736842105263157e-05,
      "loss": 2.3668,
      "step": 94
    },
    {
      "epoch": 0.01270520579089906,
      "grad_norm": 0.8713761568069458,
      "learning_rate": 1.2631578947368422e-05,
      "loss": 1.6868,
      "step": 95
    },
    {
      "epoch": 0.012838944799224313,
      "grad_norm": 0.9521767497062683,
      "learning_rate": 1.0526315789473684e-05,
      "loss": 1.9575,
      "step": 96
    },
    {
      "epoch": 0.012972683807549568,
      "grad_norm": 1.0465532541275024,
      "learning_rate": 8.421052631578948e-06,
      "loss": 1.7173,
      "step": 97
    },
    {
      "epoch": 0.01310642281587482,
      "grad_norm": 0.8948158621788025,
      "learning_rate": 6.315789473684211e-06,
      "loss": 1.7095,
      "step": 98
    },
    {
      "epoch": 0.013240161824200073,
      "grad_norm": 0.909359335899353,
      "learning_rate": 4.210526315789474e-06,
      "loss": 2.1472,
      "step": 99
    },
    {
      "epoch": 0.013373900832525328,
      "grad_norm": 1.049118995666504,
      "learning_rate": 2.105263157894737e-06,
      "loss": 1.9515,
      "step": 100
    }
  ],
  "logging_steps": 1,
  "max_steps": 100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 756384106807296.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
